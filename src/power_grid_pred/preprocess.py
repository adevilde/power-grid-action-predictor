import polars as pl
import numpy as np
from typing import Tuple
from sklearn.model_selection import train_test_split

from .generate_data import DATA_DIR, RAW_DIR


PROC_DIR = DATA_DIR / "processed"
PROC_DIR.mkdir(parents=True, exist_ok=True)

RAW_FEATURES = RAW_DIR / "df_features.parquet"
RAW_TARGETS = RAW_DIR / "df_targets.parquet"
CACHED_DATASET = PROC_DIR / "dataset.parquet"


def load_raw() -> Tuple[pl.DataFrame, pl.DataFrame]:
    """
    Load the raw feature and target Parquet files generated by the simulation.

    Returns:
        Tuple of Polars DataFrames `(df_features, df_targets)` matching the raw
        outputs from `generate_data`.

    Raises:
        FileNotFoundError: If either raw Parquet file is missing.
    """
    if not RAW_FEATURES.exists() and not RAW_TARGETS.exists():
        raise FileNotFoundError(
            "Raw data not found. Run make generate_data first."
        )
    
    df_features = pl.read_parquet(RAW_FEATURES)
    df_targets = pl.read_parquet(RAW_TARGETS)
    return df_features, df_targets


def build_preprocessed_dataset(force: bool = False) -> pl.DataFrame:
    """
    Combine the raw feature/target tables into a single processed dataset and
    cache the result for faster future access. Remove any rows where at least one target (action_* column) is not finite
    (i.e. is NaN or +/- inf).

    Args:
        force: When True, rebuild the processed dataset even if a cached file exists.

    Returns:
        Polars DataFrame containing the processed dataset.
    """
    if CACHED_DATASET.exists() and not force:
        print("Using cached processed dataset.")
        return pl.read_parquet(CACHED_DATASET)
    
    df_features, df_targets = load_raw()
    target_cols = df_targets.columns

    # Compute rho_cap = max finite rho over all actions, with 10% margin.
    # We convert to numpy for simplicity.
    pdf_targets = df_targets.to_pandas()
    vals = pdf_targets.values
    finite_mask = np.isfinite(vals)

    max_finite = vals[finite_mask].max()
    rho_cap = float(max_finite + 5) # high rho for worst-case replacement

    print(f"Replacing inf in targets with rho_cap = {rho_cap:.3f}")

    # Replace non-finite values in Polars
    df_targets_clean = df_targets.with_columns(
        [
            pl.when(pl.col(c).is_finite())
            .then(pl.col(c))
            .otherwise(rho_cap)
            .alias(c)
            for c in target_cols
        ]
    )

    # Combine horizontally: one row per observation, feature columns + action target columns
    df = pl.concat([df_features, df_targets_clean], how="horizontal")

    df.write_parquet(CACHED_DATASET)
    print(f"Processed dataset saved to {CACHED_DATASET}")
    print(f"Kept {df.height} rows after removing non-finite targets.")
    return df


def train_test_split_dataset(
    test_size: float = 0.2,
    random_state: int = 0
): 
    """
    Load (or build) the processed dataset and split it into train/test sets
    compatible with scikit-learn estimators.

    Args:
        test_size: Fraction of the dataset to allocate to the test split.
        random_state: PRNG seed forwarded to `train_test_split` for reproducibility.

    Returns:
        `(X_train, X_test, y_train, y_test)` pandas DataFrames produced by
        `sklearn.model_selection.train_test_split`.
    """
    df = build_preprocessed_dataset()
    pandas_df = df.to_pandas()

    target_cols = [c for c in pandas_df.columns if c.startswith("action_")]
    feature_cols = [c for c in pandas_df.columns if c not in target_cols]

    X = pandas_df[feature_cols]
    y = pandas_df[target_cols]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

    return X_train, X_test, y_train, y_test
    

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--force", default="False", help="Force regeneration of raw data")
    args = parser.parse_args()
    # Convert "True"/"False" to a boolean
    force_flag = str(args.force).lower() == "true"

    df = build_preprocessed_dataset(force=force_flag)
    print(df.head())
