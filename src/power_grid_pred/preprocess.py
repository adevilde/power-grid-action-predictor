import polars as pl
import numpy as np
from typing import Tuple

from .generate_data import DATA_DIR, RAW_DIR


PROC_DIR = DATA_DIR / "processed"
PROC_DIR.mkdir(parents=True, exist_ok=True)

RAW_FEATURES = RAW_DIR / "df_features.parquet"
RAW_TARGETS = RAW_DIR / "df_targets.parquet"
CACHED_DATASET = PROC_DIR / "dataset.parquet"


def load_raw() -> Tuple[pl.DataFrame, pl.DataFrame]:
    """
    Load the raw feature and target Parquet files generated by the simulation.

    Returns:
        Tuple of Polars DataFrames `(df_features, df_targets)` matching the raw
        outputs from `generate_data`.

    Raises:
        FileNotFoundError: If either raw Parquet file is missing.
    """
    if not RAW_FEATURES.exists() and not RAW_TARGETS.exists():
        raise FileNotFoundError(
            "Raw data not found. Run make generate_data first."
        )
    
    df_features = pl.read_parquet(RAW_FEATURES)
    df_targets = pl.read_parquet(RAW_TARGETS)
    return df_features, df_targets


def build_preprocessed_dataset(force: bool = False) -> pl.DataFrame:
    """
    Combine the raw feature/target tables into a single processed dataset and
    cache the result for faster future access. Remove any rows where at least one target (action_* column) is not finite
    (i.e. is NaN or +/- inf).

    Args:
        force: When True, rebuild the processed dataset even if a cached file exists.

    Returns:
        Polars DataFrame containing the processed dataset.
    """
    if CACHED_DATASET.exists() and not force:
        print("Using cached processed dataset.")
        return pl.read_parquet(CACHED_DATASET)
    
    df_features, df_targets = load_raw()
    target_cols = df_targets.columns

    # Compute rho_cap = max finite rho over all actions, with 10% margin.
    # We convert to numpy for simplicity.
    pdf_targets = df_targets.to_pandas()
    vals = pdf_targets.values
    finite_mask = np.isfinite(vals)

    max_finite = vals[finite_mask].max()
    rho_cap = float(max_finite + 5) # high rho for worst-case replacement

    print(f"Replacing inf in targets with rho_cap = {rho_cap:.3f}")

    # Replace non-finite values in Polars
    df_targets_clean = df_targets.with_columns(
        [
            pl.when(pl.col(c).is_finite())
            .then(pl.col(c))
            .otherwise(rho_cap)
            .alias(c)
            for c in target_cols
        ]
    )

    # Combine horizontally: one row per observation, feature columns + action target columns
    df = pl.concat([df_features, df_targets_clean], how="horizontal")

    df.write_parquet(CACHED_DATASET)
    print(f"Processed dataset saved to {CACHED_DATASET}")
    print(f"Kept {df.height} rows after removing non-finite targets.")
    return df
    

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--force", default="False", help="Force regeneration of raw data")
    args = parser.parse_args()
    # Convert "True"/"False" to a boolean
    force_flag = str(args.force).lower() == "true"

    df = build_preprocessed_dataset(force=force_flag)
    print(df.head())
