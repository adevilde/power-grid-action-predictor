import polars as pl
from typing import Tuple
from sklearn.model_selection import train_test_split

from .generate_data import DATA_DIR, RAW_DIR


PROC_DIR = DATA_DIR / "processed"
PROC_DIR.mkdir(parents=True, exist_ok=True)

RAW_FEATURES = RAW_DIR / "df_features.parquet"
RAW_TARGETS = RAW_DIR / "df_targets.parquet"
CACHED_DATASET = PROC_DIR / "dataset.parquet"


def load_raw() -> Tuple[pl.DataFrame, pl.DataFrame]:
    """
    Load the raw feature and target Parquet files generated by the simulation.

    Returns:
        Tuple of Polars DataFrames `(df_features, df_targets)` matching the raw
        outputs from `generate_data`.

    Raises:
        FileNotFoundError: If either raw Parquet file is missing.
    """
    if not RAW_FEATURES.exists() and not RAW_TARGETS.exists():
        raise FileNotFoundError(
            "Raw data not found. Run make generate_data first."
        )
    
    df_features = pl.read_parquet(RAW_FEATURES)
    df_targets = pl.read_parquet(RAW_TARGETS)
    return df_features, df_targets


def build_preprocessed_dataset(force: bool = False) -> pl.DataFrame:
    """
    Combine the raw feature/target tables into a single processed dataset and
    cache the result for faster future access. Remove any rows where at least one target (action_* column) is not finite
    (i.e. is NaN or +/- inf).

    Args:
        force: When True, rebuild the processed dataset even if a cached file exists.

    Returns:
        Polars DataFrame containing the processed dataset.
    """
    if CACHED_DATASET.exists() and not force:
        print("Using cached processed dataset.")
        return pl.read_parquet(CACHED_DATASET)
    
    df_features, df_targets = load_raw()
    target_cols = df_targets.columns

    keep_cols = []
    for c in target_cols:
        has_finite = df_targets.select(pl.col(c).is_finite().any()).item()
        if has_finite:
            keep_cols.append(c)

    dropped_cols = set(target_cols) - set(keep_cols)
    if dropped_cols:
        print(f"Dropping {len(dropped_cols)} action columns with only non-finite values:")
        print(", ".join(sorted(dropped_cols)))

    df_targets = df_targets.select(keep_cols)
    target_cols = keep_cols

    # Combine horizontally: one row per observation, feature columns + action target columns
    df = pl.concat([df_features, df_targets], how="horizontal")

    if target_cols:
        df = df.filter(
            pl.all_horizontal([pl.col(c).is_finite() for c in target_cols])
        )
    else:
        raise ValueError("No target columns left after filtering non-finite actions.")

    df.write_parquet(CACHED_DATASET)
    print(f"Processed dataset saved to {CACHED_DATASET}")
    print(f"Kept {df.height} rows after removing non-finite targets.")
    return df


def train_test_split_dataset(
    test_size: float = 0.2,
    random_state: int = 0
): 
    """
    Load (or build) the processed dataset and split it into train/test sets
    compatible with scikit-learn estimators.

    Args:
        test_size: Fraction of the dataset to allocate to the test split.
        random_state: PRNG seed forwarded to `train_test_split` for reproducibility.

    Returns:
        `(X_train, X_test, y_train, y_test)` pandas DataFrames produced by
        `sklearn.model_selection.train_test_split`.
    """
    df = build_preprocessed_dataset()
    pandas_df = df.to_pandas()

    target_cols = [c for c in pandas_df.columns if c.startswith("action_")]
    feature_cols = [c for c in pandas_df.columns if c not in target_cols]

    X = pandas_df[feature_cols]
    y = pandas_df[target_cols]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

    return X_train, X_test, y_train, y_test
    

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--force", default="False", help="Force regeneration of raw data")
    args = parser.parse_args()
    # Convert "True"/"False" to a boolean
    force_flag = str(args.force).lower() == "true"

    df = build_preprocessed_dataset(force=force_flag)
    print(df.head())
